
So we want to have an algorithm that can learn complex tasks (like playing games), by rewarding it when it does well and punishing it when it does badly.
One algorithm for doing this is called DQN, (a little about when it was invented and atari)
At the core of this algorithm is the Q function. [show on screen] 
The Q function takes as input an observation [expand function], and a potential action, and outputs what it predicts the reward to be if it took that action in that context. The interesting thing about the q function is that it doesn't just predict the reward it will be given in the next timestep, it predicts the sum of the reward that will be given for the rest of the game.

For example, let's look at what the Q function would do in the snake game. We'll assume that the the agent gets one point each time it eats some food, and that this score is used as the reward. At each timestep, the agent evaluates the Q function for each action it could take. I'll explain why the predicted rewards aren't necessarily integers in a minute.
It then chooses the action with the highest score and executes it. 
So that part of the algorithm is pretty simple. It's just predict each score, choose the best, do it. The tricky part is how to train the Q function so that it outputs the correct predictions.

In deep Q learning, the Q function is approximated with a neural network. A neural network can be thought of as a universal function approximator, that is trained to approximate any function just by giving it input examples which are paired with a target. For details on how this works, see 3b1b. 
So how are we going to get target predictions? Or to put it another way, how can the algorithm figure out, in retrospect, what it should have predicted the reward to be at each point in the game.

This graph represents the target rewards over an enitre game



Why this video?
I did a neural networks course last year, and most of the concepts were taught to us with by 3 hour power points filled with equations. [obvious visual, information dense]. This isn't a *terrible* way of teaching, and I think I did learn a lot, but there were several algorithms that lent themselves to a more visual explanation and intuition. This is my attempt to explain deep Q learning in a more intuitive way, inspired by the explanations of Grant at 3Blue1Brown, and also using his software. [Thanks] 

Why do you want to know about Q learning?
A few reasons: I think it's the next level up from using neural networks for basic recognition tasks. It's a way more interesting way to use neural networks. It's a good intro to reinforcement learning in general. And if you understand this, you should have most of the tools necessary to understand 

Q learning is one way to create an 'agent', which is software that makes decisions. It can be used to play games like go and chess, various simpler games like balancing, or complex tasks like walking or picking up an object. Something vauguely like this is probably what they use for self-driving cars. {Maybe just say more complex tasks than classification}.

A Q Learning algorithm makes decisions by observing the state of the its environment, then predicting how much reward it will recieve for each of it's possible actions. It then chooses the action it predicts is best. 
In this example {describe based on animation of fully trained cartpole}
{decribe how reward is given at each step}

The heart of Q Learning is the way it learns to do this prediction. 
Q function
The agent is given some reward for every action it takes. {example} This leads it to 
One way you might do this is to just train the neural network by giving it observations as input and the amount of reward that ended up being given at the end of the [trial]. I tried this, and it didn't seem to be learning anything. The agent would have trouble being able to tell which actions lead to more reward and which were mistakes. The way this problem is solved is by having the network trained on it's *own* future predictions. This is a weird thing to do, {hopefully some animation helps}

EXPERIENCE REPLAY
The knowledge trickles down from the future to the past


GAMMA
network doesn't have to calculate as far into the future, only gives a fuck about the near future. Good for reducing computation and model complexity
